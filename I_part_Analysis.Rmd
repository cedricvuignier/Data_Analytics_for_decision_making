

```{r include=FALSE}

library(here)
library(tidyverse)
library(inspectdf)
library(DataExplorer)
library(GGally)
library(DT)
library(gridExtra)
require(ggplot2)
library(caret)
library(broom)
library(kableExtra)
library(car)
library(ROCR)
library(modelr)
library(RColorBrewer)
library(rattle)
library(NeuralNetTools)
library(grid)


draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'Class1', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Class2', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'Class1', cex=1.2, srt=90)
  text(140, 335, 'Class2', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  
```


# Data preparation
We see that the database is composed of 32 variables and 1000 observations. 
```{r message=FALSE, warning=FALSE}
german<- read.csv("Data/GermanCredit.csv", sep = ";")

#Renaming OBS. in OBS
colnames(german)[1] <- "OBS"

#Col names in lower case 
colnames(german) <- tolower(colnames(german))

#Inspecting the data frame, there are numerical and categorical values
# head(german)
# str(german)
# summary(german)
datatable(german)

#Creating a new variable in order to know if credit risk is good  (1) or bad (0)
german$risk <- ifelse(german$response == 1, "good", "bad") 
german$risk <- as.factor(german$risk)
german <- german %>% select(-response) #We remove the binary variable response because we will use the caterogical one (risk) that we previously created
# is.factor(german$risk) Uncomment to know if the risk variable is a factor

```

# EDA

We check for missing data. No missing data is observed. 

```{r message = FALSE, warning = FALSE}
library(Amelia)
missmap(german)
# colSums(is.na(german))

```

## Bad vs Good
In general, our model will predict many more good customers 

```{r message = FALSE, warning = FALSE}
vs<-inspect_cat( select(german, risk)) 
show_plot(vs)
```

## Anomalies
We detected some anomalies in the database: 

- guarantor: Max = 2 which is a mistake is should be 1 instead.
- education: Min = -1 which is a mistake. It should be 1 instead.
- age: Max = 125 which is probably a mistake. It should be 75 instead.

*We will therefore modify the database with the correct information in order to optimize our future models.*

```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(german$age)
summary(german$education)
summary(german$guarantor)
summary(german$duration)
summary(german$chk_acct)

#data transformation
german$age[537] <-75 
german$education [37] <- 1
german$guarantor [234] <- 1
```

```{r message=FALSE, warning=FALSE, include=FALSE}
#Here, we rename the variable history
german_EDA <- german

german_EDA$job <- factor(german_EDA$job, levels = 0:3, labels=c("unemployed/unskilled", 
     "unskilled - resident", 
     " skilled employee/official",
     "management/self-employed/
highly qualifed employee/officer"))

german_EDA$sav_acct <- factor(german_EDA$sav_acct, levels = 0:4, labels=c("< 100 DM", 
     "100 ≤ ... < 500 DM", 
     "500 ≤ ··· < 1000 DM",
     "≥ 1000 DM",
     "unknown/no savings account"))
```


```{r message = FALSE, warning = FALSE}
ggplot(german_EDA,aes(x=job,fill = job)) +
  geom_bar()+
  ggtitle("Type of Jobs", subtitle = "the majority are skilled employee")+
theme_bw()+
      theme(axis.text = element_text(size=12,face = "bold"),
            legend.text = element_text(size = 12),
            axis.title.x = element_blank(),
            axis.title.y = element_blank(),
            panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(),
            panel.border = element_blank(),
            axis.line = element_line(colour = "black"),
            axis.text.x=element_blank())

ggplot(german_EDA,aes(x=sav_acct,fill=sav_acct))+
  geom_bar()+
  ggtitle("Types of savings account", subtitle = "the majority have less than 100 DM")+
  theme_bw()+
      theme(axis.text = element_text(size=12,face = "bold"),
            legend.text = element_text(size = 12),
            axis.title.x = element_blank(),
            axis.title.y = element_blank(),
            panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(),
            panel.border = element_blank(),
            axis.line = element_line(colour = "black"),
            axis.text.x=element_blank())

```

## Sex analysis

```{r message = FALSE, warning = FALSE}
#create a variable sex 
german_EDA <- german_EDA %>% mutate(sex = ifelse(male_div == 1 | male_single == 1 | male_mar_or_wid == 1, 1,0))

german_EDA$sex <- factor(german_EDA$sex, levels = 0:1, labels=c("female", 
     "male"))


german_EDA %>% select(sex,risk) %>% group_by(sex,risk) %>% count() %>%   
  ggplot(aes(x = sex, y = n,fill = risk)) + 
  geom_bar(position = 'dodge', stat='identity') +
  geom_text(aes(label=n), position=position_dodge(width=0.9), vjust=-0.25)+
  ggtitle("Risk analysis by gender", subtitle = "the data set is not well balanced")

```

## Boxplot

```{r message = FALSE, warning = FALSE}
b1 <- german %>% ggplot(aes(x = risk, y = duration)) +
  geom_boxplot(outlier.colour="red", outlier.shape=8,
                outlier.size=1) 

b2 <- german %>% ggplot(aes(x = risk, y = amount)) +
  geom_boxplot(outlier.colour="red", outlier.shape=8,
                outlier.size=1)  
  
b3 <- german %>% ggplot(aes(x = risk, y = age)) +
  geom_boxplot(outlier.colour="red", outlier.shape=8,
                outlier.size=1)

grid.arrange(b1, b2, b3, nrow=1,
             top = textGrob("Box Plot analysis",gp=gpar(fontsize=15,font=2)))
```

*duration*
At-risk clients have longer credit duration 
*amount*
The median amounts are slightly higher for the at-risk category 
*age*
The median for good payers is higher.


```{r message = FALSE, warning = FALSE}
g1 <-  ggplot(german, aes(x = age)) + 
  geom_histogram(bins = 20, fill = 'pink', colour = 'black') + 
  ggtitle('Age distribution') + 
  xlab('Age') +
  ylab('Frequency')

g2 <- ggplot(german, aes(x = amount)) + 
  geom_histogram(bins = 20, fill = 'turquoise', colour = 'black') + 
  ggtitle('Amount distribution') + 
  xlab('Credit_Amount') +
  ylab('Frequency')

g3 <- ggplot(german, aes(x = duration)) + 
  geom_histogram(bins = 20, fill = 'lightgoldenrod', colour = 'black') + 
  ggtitle('Duration distribution') + 
  xlab('Duration') +
  ylab('Frequency')

grid.arrange(g1, g2, g3, nrow=1)

```

We can see that age, amount and duration are right skewed. In order to change it, we use the log to transform them into a normal distribution. We also note that there is an error in the age variable (125 years -> 75 years).  

```{r message = FALSE, warning = FALSE}
#Transforming variables
g1 <-  ggplot(german, aes(x = log1p(age))) + 
  geom_histogram(bins = 20, fill = 'pink', colour = 'black') + 
  ggtitle('Age distribution') + 
  xlab('Age') +
  ylab('Frequency')

g2 <- ggplot(german, aes(x = log1p(amount))) + 
  geom_histogram(bins = 20, fill = 'turquoise', colour = 'black') + 
  ggtitle('Amount distribution') + 
  xlab('Credit_Amount') +
  ylab('Frequency')

g3 <- ggplot(german, aes(x = log1p(duration))) + 
  geom_histogram(bins = 20, fill = 'lightgoldenrod', colour = 'black') + 
  ggtitle('Duration distribution') + 
  xlab('Duration') +
  ylab('Frequency')


grid.arrange(g1, g2, g3, nrow=1)
#Even if age is still a bit right skewed, it is much better than before
```

Even if age is still a bit right skewed, it is much better than before.

## Correlation

There is a correlation between amount and duration. This makes sense. 

```{r message = FALSE, warning = FALSE}
correlation_good <- german %>% filter(risk == "good") %>% select(amount, age, duration) 

ggpairs(correlation_good)

correlation_bad <- german %>% filter(risk == "bad") %>% select(amount, age, duration) 

ggpairs(correlation_bad)
```

# Data modeling

## Data splitting and data balancing

Because we have seen in the EDA that age, amount and duration are right skewed, we transformed the data with the log. Moreover, we also scale the data in oder to normalize them.

We split the dataset into a training test and a test set. The training set is used to find optimal hyperparameters when tuning them. For this process, we will use a 5-fold cross-validation applied on the train set. The test set will be used to compare and evaluate the models in order to know how well the models can generalize new data. 

Therefore, we split the data set into two parts: 

- Training set (80%)
- Test set (20%)

```{r message = FALSE, warning = FALSE}
#First we tried to scale the data to normalize them 
german <- german %>% mutate(LogAge = log(age),
                    Log1pAmount = log1p(amount), 
                    Log1pDuration = log1p(duration))

german$LogAgestd <- scale(german$LogAge)
german$Log1pAmountstd <- scale(german$Log1pAmount)
german$Log1pDurationstd <- scale(german$Log1pDuration)


german <- german %>% select(-age, -amount, -obs, -duration, -LogAge, -Log1pAmount, -Log1pDuration) 


#Splitting the dataset into a training (80%) and a test set (20%)
set.seed(245156)
index <- sample(x=1:2, size = nrow(german), replace = TRUE, prob = c(0.8, 0.2))
german.tr <- german[index == 1,]
german.te <- german[index == 2,]
table_tr <- table(german.tr$risk)
table_te <- table(german.te$risk)
accuracy <- table_te["good"]/sum(table_te) #% of reponses = good in data set

```

In the EDA, we saw an unbalanced dataset, we have mmuch more credits classified as good (700) than bad (300). In order to have a correct prediction, we need to balance the number of good et bad credits in our training set. It will thus improve the sensitivity and the specificity. Because the importance is to predict well risky credits in order to avoid huge losses whithin the company, we need a good specificity but the balance between both measures is important.

As we can see with the risk variable, the data are unbalanced

```{r message = FALSE, warning = FALSE}
#As we can see with the risk variable, the data are unbalanced
table(german.tr$risk) %>% kable(align = "c", col.names = c("Variable", "Frequence"))
```

Below, we display the number of bad and good credits after having balanced the training set. 

```{r message = FALSE, warning = FALSE}
#Balancing the data 
# summary(german.tr)
n.bad <- sum(german.tr$risk == "bad")
n.good <- sum(german.tr$risk == "good")

set.seed(245156)
index.bad <- which(german.tr$risk == "bad")
index.good <- sample(x = which(german.tr$risk == "good"), size = n.bad, replace = FALSE)

german.tr.bal <- german.tr[c(index.bad, index.good),]

table(german.tr.bal$risk) %>% kable(align = "c", col.names = c("Variable", "Frequence"))

```
## Accuracy metric: training and fitting the models

By using the CARET package, we need some paramaters first to train our models. It will be used for each model with accuracy metric.

As mentioned before, we will use a five cross-validation to subset the training set (into a validation set) in order to asses how well the model will generalize to the test set.  

```{r}
train_control <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"
```


### Logistic regression 

A logistic regression is a binary classifier and is used here for output with two classes.

We will use the AIC selection in order to have a model with more significant variables.


```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
set.seed(123)

fit_glm_AIC = train(
  form = risk ~ .,
  data = german.tr.bal,
  trControl = train_control,
  method = "glmStepAIC",
  metric = metric,
  family = "binomial"
)
```


```{r message = FALSE, warning = FALSE}
pred_glm_aic <- predict(fit_glm_AIC, newdata = german.te)
cm_glm_aic <- confusionMatrix(data = pred_glm_aic, reference = german.te$risk)
draw_confusion_matrix(cm_glm_aic)

```

By analysing more precisely our model, we observe that our model contains insignificant variables according to the p-valus. We decided to remove them from the model. 

```{r message = FALSE, warning = FALSE}
#Using a function to have a good layout for the significance of our parameters
pval_star <- function(p, cutoffs = c(0.05, 0.01, 0.001)) {
  stopifnot(length(cutoffs) == 3)
  if (length(p) > 1) {
    sapply(p, pval_star, cutoffs = cutoffs)
  }
  else {
    ifelse(p > cutoffs[1], "", ifelse(p > cutoffs[2],
      " *",
      ifelse(p > cutoffs[3],
        " **",
        " ***"
      )
    ))
  }
}

pvalues<- fit_glm_AIC$finalModel
pvalues %>% tidy() %>%
  select(-statistic) %>%
  mutate(p.value = paste(round(p.value, 4), pval_star(p.value))) %>%
  kable(digits = 3,
    col.names = c("Parameter", "Estimate", "Standard Error", "P.value")
  ) %>%
  kable_styling(bootstrap_options = "striped")

```

The logistic regression has no parameter to tune unlike other models that we will use. 

```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
#Logistic regression
set.seed(123)

fit_glm_AIC = train(
  form = risk ~ chk_acct + history + used_car + education + sav_acct + employment + male_single +
  prop_unkn_none + rent + job + foreign + Log1pDurationstd,
  data = german.tr.bal,
  trControl = train_control,
  method = "glmStepAIC",
  metric = metric,
  family = "binomial"
)
```


```{r message = FALSE, warning = FALSE}
pred_glm_aic <- predict(fit_glm_AIC, newdata = german.te)
cm_glm_aic <- confusionMatrix(data = pred_glm_aic, reference = german.te$risk)
draw_confusion_matrix(cm_glm_aic)

```
Even if our final model has decreased in accuracy, the gab between sensitivity and specificity is smaller. Moreover, because we removed insignificant variables, our model makes more sense. 

Finally, we compute the VIF coefficients to look for multicollineratity which is not the case as we can see below (VIF coefficients <5).

```{r message = FALSE, warning = FALSE}
vif(pvalues) %>% kable(col.names = "VIF Coefficient") %>%  kable_styling(bootstrap_options = "striped")
```


### Nearest neighbour classification

The K-NN model compute the distance between observations in order to classify them. It means that the model counts the classes of the K nearest instances and the class with the most observations is selected for the K observations. 


```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
#KNN
set.seed(456)
fit_knn_tuned = train(
  risk ~ .,
  data = german.tr.bal,
  method = "knn",
  metric = metric,
  trControl = train_control,
  tuneGrid = expand.grid(k = seq(1, 101, by = 1))
)

```

We are looking for the optimal K.

```{r message = FALSE, warning = FALSE}
plot(fit_knn_tuned)


K <- fit_knn_tuned$finalModel$k
paste("The optimal K of our model is", K) %>% kable(col.names = NULL, align = "l")
# K %>% kable(col.names = "K", align = "c")
```


```{r message = FALSE, warning = FALSE}
pred_knn <- predict(fit_knn_tuned, newdata = german.te)
cmknn <- confusionMatrix(data = pred_knn, reference = german.te$risk)
draw_confusion_matrix(cmknn)
```


### Support Vector Machine (SVM)

The model, considered as a separation method, consists in looking for the linear optimal separation of the hyperplane in order to classify the observations. 

With the CARET package, we can only tune the cost hyperparameter. The cost controls the tolerance to bad classification and corresponds to the smoothing of the border that classifies the observations. The larger is the cost (border is not smooth), the fewer missclassifications are allowed. If the cost is too large, it can lead to overfitting. 

```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
#SVM
hp_svm <- expand.grid(cost = 10 ^ ((-2):1))
set.seed(1953)
fit_svm <- train(
  form = risk ~ .,
  data = german.tr.bal,
  trControl = train_control,
  tuneGrid = hp_svm,
  method = "svmLinear2",
  metric = metric
)
```


```{r}
C <- fit_svm$finalModel$cost
paste("The optimal cost for our model is", C) %>% kable(col.names = NULL, align="l")
```


```{r message = FALSE, warning = FALSE}
pred_svm <- predict(fit_svm, newdata = german.te)
cmsvm <- confusionMatrix(data = pred_svm, reference = german.te$risk)
draw_confusion_matrix(cmsvm)
```

### Neural network

A neural network combines several predictions of small nodes and contains lots of coefficients (weights) and parameters to tune. Neural networks are over-parametrized by nature and therefore, the solution is quite unstable. In order to avoid overfitting, we will use a weight decay to penalise the largest weights. We will also tune the size of the neural networks to find the opimal one. 

```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
#Neural network
hp_nn <- expand.grid(size = 2:10,
                     decay = seq(0, 0.5, 0.05))
set.seed(2006)
fit_nn <- train(
  form = risk ~ .,
  data = german.tr.bal,
  trControl = train_control,
  tuneGrid = hp_nn,
  method = "nnet",
  metric = metric
)
```

Below, we display our neural network.

```{r message = FALSE, warning = FALSE, fig.height= 15, fig.width=15}
plotnet(fit_nn$finalModel)

decay <- fit_nn$finalModel$decay
paste("The best neural networs has a decay of", decay) %>% kable(col.names = NULL, align="l")


arch<- fit_nn$finalModel$n
cat("The model architecture is the following:","\n", "- Number of layers =", length(arch), "\n", 
      "- Nodes in the first layer =", arch[1], "\n",
      "- Nodes in the second layer =", arch[2], "\n",
      "- Nodes in the third layer (output) =", arch[3]) %>% kable(col.names = NULL, align="l")


```


```{r message = FALSE, warning = FALSE}
pred_nn <- predict(fit_nn, newdata = german.te)
cmnn <- confusionMatrix(data = pred_nn, reference = german.te$risk)
draw_confusion_matrix(cmnn)
```

### Linear discriminant analysis (LDA)

The aim of the linear discriminant analysis is to explain and predict the class of an observation according to a linear combination of features that characterizes the classes.

There is no tunige parameter for this model. 

```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
#LDA
set.seed(1839)
fit_LDA <- train(risk ~ .,
                 data = german.tr.bal,
                 method = "lda",
                 metric = metric,
                 trControl = train_control)

```


```{r message = FALSE, warning = FALSE}
pred_lda <- predict(fit_LDA, newdata = german.te)
cmlda <- confusionMatrix(data = pred_lda, reference = german.te$risk)
draw_confusion_matrix(cmlda)
```

### Random Forest

A random forest model is composed by a multitude of decision trees forming a whole. Each individual tree predicts a class and the class receiving the most votes becomes the prediction model. Thus, the predictions of the individual trees are averaged to get a final prediction. The trees are uncorrelated and operate as a set. Therefore they outperform the individual models. The non-correlation between trees brings some stability to the final prediction.

To tune the model, we will play around the number of trees composing the random forest.


```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
#Random forest 
hp_rf <- expand.grid(.mtry = (1:15)) 
set.seed(531)
fit_rf <- train(
  risk ~ .,
  data = german.tr.bal,
  method = 'rf',
  metric = metric,
  trControl = train_control,
  tuneGrid = hp_rf
)
```

Here, we display the importance of the variable. 

```{r message = FALSE, warning = FALSE}
varImp(fit_rf) %>% plot()
```



```{r message = FALSE, warning = FALSE}
ntree <- fit_rf$finalModel$mtry
paste("Our random forest model is composed by", ntree, "trees") %>% kable(col.names = NULL, align="l")
```


```{r message = FALSE, warning = FALSE}
pred_rf <- predict(fit_rf, newdata = german.te)
cmrf <- confusionMatrix(data = pred_rf, reference = german.te$risk)
draw_confusion_matrix(cmrf)
```

### Decision tree

A tree is a graphical representation of a set of rules. The importance of characteristics is clear and relationships can be interpreted.

For the decision tree model, we decided to tune the complexity parameter "cp" to be in the range 0.03 and 0.

The complexity parameter control the optimal size of the tree. If the cost of adding another variable to the tree from the current node is above the value of the complexity parameter, we stop growing the tree. Therefore, the complexity parameter is the minimum improvement needed in the model at each node. 

Because we use the CARET package, we do not need to prune the tree as the tree is already simplified. 

```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
#Decision tree
hp_ct <- expand.grid(cp = seq(from = 0.03, to = 0, by = -0.003))
set.seed(1851) #allow repoducibility of the results
fit_ct <- train(
  form = risk ~ .,
  data = german.tr.bal,
  trControl = train_control,
  tuneGrid = hp_ct,
  method = "rpart",
  metric = metric
)
```

Below, we diplay our final tree. The most important variable is the first used to build the tree. For our tree, the most important variable is "chk_acct" which corresponds to the checking account status. 

```{r message = FALSE, warning = FALSE, results=FALSE}
fancyRpartPlot(fit_ct$finalModel, main = "Regression tree", caption = NULL)
```

The confusion matrix demonstrates that sensitivity and specificity are well balanced.

```{r message = FALSE, warning = FALSE}
pred_ct <- predict(fit_ct, newdata = german.te)

cmct <- confusionMatrix(data = pred_ct, reference = german.te$risk)
draw_confusion_matrix(cmct)
```

### Naive Bayes


Naive Bayes model uses a probabilistic approach and allows an interpretation of the features.

The aime is to predict the class of an instance by choosing the class that maximizes the conditional probability given the features. 

In order to find the optimal model, we will try to use or not use the kernel density and we will set the laplace smoother value to zero. In addition, the adjust parameter will allow us to adjust the bandwidth of the kernel density. The larger is the number, the more flexible is the density estimate. 

```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
#Naive Bayes
hp_nb <- expand.grid(
  usekernel = c(TRUE, FALSE),
  laplace = 0,
  adjust = seq(from = 0.1, to = 5, by = 0.5)
)

set.seed(2013)
fit_nb <- train(
  form = risk ~ .,
  data = german.tr.bal,
  trControl = train_control,
  tuneGrid = hp_nb,
  method = "naive_bayes",
  metric = metric
)
```

Our best model uses kernel density as we can see on the following graph.


```{r message = FALSE, warning = FALSE}
plot(fit_nb)
```


```{r message = FALSE, warning = FALSE}
pred_nb <- predict(fit_nb, newdata = german.te)
cmnb <- confusionMatrix(data = pred_nb, reference = german.te$risk)
draw_confusion_matrix(cmnb)
```


**Recapitulations of the accuracy results**

```{r message = FALSE, warning = FALSE}
accuracy<- c(

confusionMatrix(predict.train(fit_glm_AIC, newdata = german.te),
german.te$risk)$overall[1],
confusionMatrix(predict.train(fit_nb, newdata = german.te),
german.te$risk)$overall[1],
confusionMatrix(predict.train(fit_nn, newdata = german.te),
german.te$risk)$overall[1],
confusionMatrix(predict.train(fit_rf, newdata = german.te),
german.te$risk)$overall[1],
confusionMatrix(predict.train(fit_svm, newdata = german.te),
german.te$risk)$overall[1], 
confusionMatrix(predict.train(fit_LDA, newdata = german.te),
german.te$risk)$overall[1], 
confusionMatrix(predict.train(fit_ct, newdata = german.te),
german.te$risk)$overall[1], 
confusionMatrix(predict.train(fit_knn_tuned, newdata = german.te),
german.te$risk)$overall[1]
)

model<- c("fit_glm_AIC", "fit_nb", "fit_nn", "fit_rf", "fit_svm", "fit_LDA", "fit_ct", "fit_knn_tuned")

results<- tibble(accuracy, model) %>% arrange(desc(accuracy))

results %>% kable() %>% kable_styling()

best <-
results %>% 
  slice_max(accuracy)
  

paste("According to the accuracy, the best model is", best$model,"with" ,"accuracy =", round(best$accuracy, digits = 4)) %>% kable(col.names = NULL, align="l")

```


## ROC metric: training and fitting the models 

In this part, we reproduce the same models as before but instead of the accuracy, we will use the ROC metric to evaluate the models and the Leave-One-Out Cross-Validation (LOOCV) method to assess the tuned hyperparameters.

```{r message = FALSE, warning = FALSE, results=FALSE}
train_control <- trainControl(method = "LOOCV",
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

metric <- "ROC"
```

We first train the model on the training set, then we make the predictions on the test set. 

```{r message = FALSE, warning = FALSE, results=FALSE, cache=TRUE}
#Logistic regression
set.seed(123)
fit_glm_AIC = train(
  form = risk ~ chk_acct + history + used_car + education + sav_acct + employment + male_single +    prop_unkn_none + rent + job + foreign + Log1pDurationstd,
  data = german.tr.bal,
  trControl = train_control,
  method = "glmStepAIC",
  metric = metric,
  family = "binomial"
)

save(fit_glm_AIC, file="fit_glm_AIC")

#KNN
set.seed(456)
fit_knn_tuned = train(
    risk ~ .,
    data = german.tr.bal,
    method = "knn",
    metric = metric,
    trControl = train_control,
    tuneGrid = expand.grid(k = seq(1, 101, by = 1))
)

save(fit_knn_tuned, file="fit_knn_tuned")

#SVM
hp_svm <- expand.grid(cost = 10 ^ ((-2):1))
set.seed(1953)
fit_svm <- train(
    form = risk ~ .,
    data = german.tr.bal,
    trControl = train_control,
    tuneGrid = hp_svm,
    method = "svmLinear2",
    metric = metric
)

save(fit_svm, file="fit_svm")

#Neural network
hp_nn <- expand.grid(size = 2:10,
                     decay = seq(0, 0.5, 0.05))
set.seed(2006)
fit_nn <- train(
    form = risk ~ .,
    data = german.tr.bal,
    trControl = train_control,
    tuneGrid = hp_nn,
    method = "nnet",
    metric = metric
)

save(fit_nn, file="fit_nn")

#Discriminant analysis
set.seed(1839)
fit_LDA <- train(risk ~ .,
                 data = german.tr.bal,
                 method = "lda",
                 metric = metric,
                 trControl = train_control)

save(fit_LDA, file="fit_LDA")

#Random forest 
hp_rf <- expand.grid(.mtry = (1:15)) 
set.seed(531)
fit_rf <- train(
    risk ~ .,
    data = german.tr.bal,
    method = 'rf',
      tuneGrid = hp_rf,
    metric = metric,
    trControl = train_control
    )

save(fit_rf, file="fit_rf")

#Decision tree 
hp_ct <- expand.grid(cp = seq(from = 0.03, to = 0, by = -0.003))
set.seed(1851)
fit_ct <- train(
    form = risk ~ .,
    data = german.tr.bal,
    trControl = train_control,
    tuneGrid = hp_ct,
    method = "rpart",
    metric = metric
)

save(fit_ct, file="fit_ct")

#Naive Bayes
hp_nb <- expand.grid(
    usekernel = c(TRUE, FALSE),
    laplace = 0,
    adjust = seq(from = 0.1, to = 5, by = 0.5)
)

set.seed(2013)
fit_nb <- train(
    form = risk ~ .,
    data = german.tr.bal,
    trControl = train_control,
    tuneGrid = hp_nb,
    method = "naive_bayes",
    metric = metric
)

save(fit_nb, file="fit_nb")
```

```{r message = FALSE, warning = FALSE}

#load("data/fit_LDA")


pred_glm_AIC_roc <- predict(fit_glm_AIC, newdata = german.te, type="prob")
pred_knn_tuned_roc <- predict(fit_knn_tuned, newdata = german.te, type="prob")
pred_svm_roc <- predict(fit_svm, newdata = german.te, type="prob")
pred_nn_roc <- predict(fit_nn, newdata = german.te, type="prob")
pred_lda_roc <- predict(fit_LDA, newdata = german.te, type="prob")
pred_rf_roc <- predict(fit_rf, newdata = german.te, type="prob")
pred_ct_roc <- predict(fit_ct, newdata = german.te, type="prob")
pred_nb_roc <- predict(fit_nb, newdata = german.te, type="prob")
```


**Recapitulations of the ROC results**

We plot the ROC curve, which is simply a plot of the values of sensitivity against one minus specificity, as the value of the cut-point c is increased from 0 through to 1. A perfect model would predict perfectly the (sensitivity), i.e. reaching 100% of correct answer. At the same time, it would make no mistake for predicting negative answers (1-specificity).
As a result, a perfect model would reach the upper left corner of our ROC graph. It means that we can estimate the model quality by computing the area of the ROC curve above the purely random model represented by the straight line. If this area is high, we have a good discrimination level. The area takes value between 0.5 and 1. A value above 0.8 would be
considered as a good level.

```{r message = FALSE, warning = FALSE}
#List of predictions
preds_list <- list(
    pred_glm_AIC_roc[,2],
    pred_knn_tuned_roc[,2],
    pred_svm_roc[,2],
    pred_nn_roc[,2],
    pred_lda_roc[,2],
    pred_rf_roc[,2], 
    pred_ct_roc[,2],
    pred_nb_roc[,2])
#List of actual values (same for all)
m <- length(preds_list)
actuals_list <- rep(list(german.te$risk), m)

#Plot the ROC curves
pred <- prediction(preds_list, actuals_list)
rocs <- performance(pred, "tpr", "fpr")
plot(rocs, col = as.list(1:m), main = "Test Set ROC Curves") %>% legend(x = "bottomright", 
       legend = c("glm_AIC", "knn_tuned", "svm", "nn", "lda", "rf","ct", "nb"),
       fill = 1:m) %>% abline(coef = c(0,1)) 


auc <- performance(pred, measure = "auc")
auc <- auc@y.values
auc<- tibble(auc)
auc$model <- c("glm_AIC", "knn_tuned", "svm", "nn", "lda", "rf","ct", "nb")
results_auc <- as.data.frame(lapply(auc, unlist))
results_auc <- results_auc %>% arrange(desc(auc))
results_auc %>% kable() %>% kable_styling()

best_auc <-
results_auc %>% 
  slice_max(auc)

paste("According to the AUC, the best model is", best_auc$model, "with AUC =", round(best_auc$auc, digits = 4)) %>% kable(col.names = NULL, align="l")

```


Remarques du prof: corriger les valeurs abérrantes, déterminer les outliers (ne pas les enlever car cela ne résulte pas d'erreurs apparentes), faire les boxplots pour savoir quelles variables pourraient être incluses dans notre modèle, Expliquers sensitivity et specificity, faire d'autres modèles. 


DALEX

```{r}

library(DALEX)


#residuals for glm
resids_glm_AIC <- model_performance(explainer_glm_AIC)

vip_glm_AIC <- variable_importance(explainer_glm_AIC, loss_function = loss_root_mean_square) 



#training
set.seed(123)

fit_glm_AIC = train(
  form = risk ~ .,
  data = german.tr.bal,
  trControl = train_control,
  method = "glmStepAIC",
  metric = metric,
  family = "binomial"
)


y_numeric <- as.numeric(german.tr.bal$risk) -1

explainer_glm_AIC <- explain(fit_glm_AIC, data = german[,-28], y = y_numeric)

summary(explainer_glm_AIC)

#importance variable
vip_glm_AIC <- variable_importance(explainer_glm_AIC, loss_function = loss_root_mean_square) 
plot(vip_glm_AIC)
summary(vip_glm_AIC)

#residuals for glm
resids_glm_AIC <- model_performance(explainer_glm_AIC)


#Dessous j'ai essayé autrement mais c'est faux, à corriger
#prediction
    custom_predict <- function(fit_glm_AIC, german.te) {
       predict(fit_glm_AIC, german.te)$predictions
    }
    explainer_glm_AIC <- explain(fit_glm_AIC, data = german.tr.bal, y = german.tr.bal$risk,
                              predict_function = custom_predict)
    
    summary(explainer_glm_AIC)



#residuals 
custom_residual <- function(fit_glm_AIC, german.te, y, predict_function) {
       abs(y - predict_function(fit_glm_AIC, german.te))
    }
    aps_glm_AIC_exp <- explain(fit_glm_AIC, data = german.tr.bal,
                              y = german.tr.bal$risk,residual_function = custom_residual)
    
summary(aps_glm_AIC_exp)
    
    
```

